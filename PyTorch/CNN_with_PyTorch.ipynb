{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "https://adventuresinmachinelearning.com/convolutional-neural-networks-tutorial-in-pytorch/ <br>\n",
    "\n",
    "Dumoulin, V., & Visin, F. (2016). A guide to convolution arithmetic for deep learning. arXiv preprint arXiv:1603.07285.\n",
    "### Building a Convolutional Neural Network CNN classifier\n",
    "**Dataset:** MNIST (0-9 digits) <br>\n",
    "**CNN architecture:**\n",
    "1. **input:** 28x28 pixel greyscale image\n",
    "2. **1st layer: using nn.Sequential object** \n",
    "    1. a convolutional layer of 32 channels i.e. filters with each having a 5x5 convolutional kernel and stride = 1 (the convolutional filter slides one step at a time along the x and y axis) with input = 28x28 output = 32x28x28\n",
    "    2. a ReLU activation is applied on the convolution output\n",
    "    3. a Max pooling layer having a kernel size of 2x2 and stride = 2 (i.e. the pooling kernel slides 2 steps at a time in both directions x and y) with input = 32x28x28 output = 32x14x14\n",
    "3. **2nd layer: using nn.Sequential object** \n",
    "    1. a convolutional layer of 64 channels i.e. filters with each having a 5x5 convolutional kernel and stride = 1 (the convolutional filter slides one step at a time along the x and y axis) with input = 32x14x14 output = 64x14x14\n",
    "    2. a ReLU activation is applied on the convolution output\n",
    "    3. a Max pooling layer having a kernel size of 2x2 and stride = 2 (i.e. the pooling kernel slides 2 steps at a time in both directions x and y) with input = 64x14x14 output = 64x7x7\n",
    "4. flattening the output of the previous max pooing layer i.e. from (64,7,7) to (3164,1) vector s.t. 64x7x7 = 3164\n",
    "5. **3rd layer:** a fully connected layer with input = 3164 nodes and output = 1000 nodes\n",
    "6. **4th layer:** a fully connected layer with input = 1000 nodes and output = 10 nodes = the number of classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10)\n",
    "\n",
    "**NB:**\n",
    "- **nn.Sequential method** allows us to create **sequentially ordered layers** in our network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\walmasri\\\\Documents\\\\Thèse Cifre\\\\Tutorials\\\\PyTorch'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "1. **create the train and test dataset** using torchvision.datasets.MNIST(root = storage location, train = bool, transform = .transform() torchvision object, download = bool):\n",
    "    1. root = specifies the folder where the train.pt and test.pt data files exist\n",
    "    2. train = informs the data set to pickup either the train.pt data file or the test.pt data file\n",
    "    3. transform = list of transformations to be applied on the input data, Here, we use to do 2 transformations:\n",
    "        1. convert the data into a PyTorch Tensor\n",
    "        2. normalize the data into a normal distribution of mean 0.1307 and standard deviation = 0.3081 <br> **NB:** if we have many channels we need to provide the mean and std of each channel in that way:\n",
    "            1. transforms.Normalize((M1, M2, ... Mn), (Std1, Std2, ... Stdn))\n",
    "            2. the normalization formula is the following : input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "    4. download = tells the MNIST data set function to download the data (if required) from an online source\n",
    "2. **Load the dataset** using torch.utils.data.DataLoader, DataLoader has many advantages:\n",
    "    1. the ability to **shuffle** the data easily\n",
    "    2. the ability to easily **batch** the data\n",
    "    3. the abilityto make data consumption more efficient via the ability to load the data in parallel using **multiprocessing**.\n",
    "    4. A data loader can be used **as an iterator** – so to **extract the data** we can just **use** the standard Python iterators such as **enumerate**\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 6\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# set the train data directory; PyTorch will store them in this location\n",
    "DATA_PATH = './data/MNIST'\n",
    "# set the directory of the trained model; We will save the final CNN model parameters in this location \n",
    "# when training is complete\n",
    "MODEL_STORE_PATH = './models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1175e46aa842f9960d7d73d6962f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9205f74cb4e74fad92fa11469c478dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f621f8333c748338c24ea0cacb7d34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df13aede5574e7794b71646ac43738e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data/MNIST\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# transforms to apply to the data\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into batches\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CNN\n",
    "- The most straight-forward way of creating a neural network structure in PyTorch is by creating a class which **inherits from the nn.Module super class within PyTorch**\n",
    "- nn.Conv2d(nbr of input channels = 1, nbr of output channels = 32, kernel_size=(x-size=5, y-size=5), stride=1, padding= nbr of padding along x and y axis):\n",
    "    - the nbr of padding is calculated via the formula: Wout = (Win - K +2P)/S +1 here S = 1, K = 5, Win = input width = 28, Wout = output width = 28 => P = 2; it is applied for width and height but here width = height so we will do it once\n",
    "    - kernel_size=(x-size=5, y-size=5) since the x-size = y-size ==> we can write kernel_size = 5\n",
    "- nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, return_indices=False, ceil_mode=False):\n",
    "    - also here kernel_size=(x-size=2, y-size=2) since the x-size = y-size ==> we can write kernel_size = 2\n",
    "    - given width = height and following the formula: Wout = (Win - K +2P)/S +1 here S = 2, K = 2, Win = input = 28, Wout = output = 14 => P = 0;\n",
    "- **drop-out layer** to avoid **over-fitting** in the model\n",
    "- a fully connected layer nn.Linear(nbr of input nodes, nbr of output nodes)\n",
    "- **overwriting the forward function**\n",
    "\n",
    "**NB:**\n",
    "- we haven't defined a softMax activation for the final classification layer because **CrossEntropyLoss** function **combines both a SoftMax activation and a cross entropy loss** function in the same function \n",
    "- To keep track of the accuracy we need to count the nbr of positives:\n",
    "    1. **torch.max(arg1, arg2)** function returns the index of the maximum value in a tensor; it is a 10-elements tensor containing probabilities of the sample belonging to the digit i such that i = index in the tensor i.e. output.data[0,3] = probability that image sample 0 belongs to the digit 2:\n",
    "        - The first argument is the tensor to be examined\n",
    "        - the second argument is the axis over which to determine the index of the maximum \n",
    "        - The output tensor from the model will be of size (batch_size, 10). To determine the model prediction, for each sample in the batch we need to find the maximum value over the 10 output nodes. Each of these will correspond to one of the hand written digits. The output node with the highest value will be the prediction of the model. Therefore, we need to set the second argument of the torch.max() function to 1 – this points the max function to examine the output node axis (axis=0 corresponds to the batch_size dimension).\n",
    "    2. Count the nbr of correct predictions using **(predicted == labels).sum().item()**: Note the output of sum() is still a tensor, so to access it’s value you need to call **.item()**\n",
    "    3. Divide the number of correct predictions by the batch_size (equivalent to labels.size(0)) to obtain the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        # inheritance\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [100/600], Loss: 0.2169, Accuracy: 93.00%\n",
      "Epoch [1/6], Step [200/600], Loss: 0.0798, Accuracy: 98.00%\n",
      "Epoch [1/6], Step [300/600], Loss: 0.0622, Accuracy: 99.00%\n",
      "Epoch [1/6], Step [400/600], Loss: 0.2029, Accuracy: 96.00%\n",
      "Epoch [1/6], Step [500/600], Loss: 0.0321, Accuracy: 99.00%\n",
      "Epoch [1/6], Step [600/600], Loss: 0.0552, Accuracy: 97.00%\n",
      "Epoch [2/6], Step [100/600], Loss: 0.0254, Accuracy: 99.00%\n",
      "Epoch [2/6], Step [200/600], Loss: 0.0527, Accuracy: 98.00%\n",
      "Epoch [2/6], Step [300/600], Loss: 0.0062, Accuracy: 100.00%\n",
      "Epoch [2/6], Step [400/600], Loss: 0.0230, Accuracy: 100.00%\n",
      "Epoch [2/6], Step [500/600], Loss: 0.0675, Accuracy: 97.00%\n",
      "Epoch [2/6], Step [600/600], Loss: 0.0213, Accuracy: 100.00%\n",
      "Epoch [3/6], Step [100/600], Loss: 0.1210, Accuracy: 98.00%\n",
      "Epoch [3/6], Step [200/600], Loss: 0.0103, Accuracy: 100.00%\n",
      "Epoch [3/6], Step [300/600], Loss: 0.0227, Accuracy: 99.00%\n",
      "Epoch [3/6], Step [400/600], Loss: 0.0618, Accuracy: 98.00%\n",
      "Epoch [3/6], Step [500/600], Loss: 0.0705, Accuracy: 98.00%\n",
      "Epoch [3/6], Step [600/600], Loss: 0.0311, Accuracy: 100.00%\n",
      "Epoch [4/6], Step [100/600], Loss: 0.0603, Accuracy: 99.00%\n",
      "Epoch [4/6], Step [200/600], Loss: 0.1412, Accuracy: 97.00%\n",
      "Epoch [4/6], Step [300/600], Loss: 0.0936, Accuracy: 97.00%\n",
      "Epoch [4/6], Step [400/600], Loss: 0.0660, Accuracy: 98.00%\n",
      "Epoch [4/6], Step [500/600], Loss: 0.0306, Accuracy: 99.00%\n",
      "Epoch [4/6], Step [600/600], Loss: 0.0990, Accuracy: 98.00%\n",
      "Epoch [5/6], Step [100/600], Loss: 0.0171, Accuracy: 100.00%\n",
      "Epoch [5/6], Step [200/600], Loss: 0.0241, Accuracy: 99.00%\n",
      "Epoch [5/6], Step [300/600], Loss: 0.0523, Accuracy: 98.00%\n",
      "Epoch [5/6], Step [400/600], Loss: 0.0044, Accuracy: 100.00%\n",
      "Epoch [5/6], Step [500/600], Loss: 0.1420, Accuracy: 97.00%\n",
      "Epoch [5/6], Step [600/600], Loss: 0.0822, Accuracy: 98.00%\n",
      "Epoch [6/6], Step [100/600], Loss: 0.1221, Accuracy: 97.00%\n",
      "Epoch [6/6], Step [200/600], Loss: 0.0312, Accuracy: 99.00%\n",
      "Epoch [6/6], Step [300/600], Loss: 0.0618, Accuracy: 97.00%\n",
      "Epoch [6/6], Step [400/600], Loss: 0.0189, Accuracy: 99.00%\n",
      "Epoch [6/6], Step [500/600], Loss: 0.0220, Accuracy: 99.00%\n",
      "Epoch [6/6], Step [600/600], Loss: 0.0044, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        # !!!! we don’t have to call model.forward(images) as nn.Module knows that forward needs to be called when it \n",
    "        # executes model(images) \n",
    "        # !!!\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item()) # The loss is appended to a list that will be used later to plot the progress of the training\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "1. **!** Set the model to ***evaluation mode*** by running **model.eval()**: This is a handy function which **disables any drop-out or batch normalization** layers in your model, which will befuddle your model evaluation / testing. \n",
    "2. The **torch.no_grad()** statement **disables the autograd functionality** in the model as it is **not needed in model testing / evaluation**, and this will act to **speed up the computations**. \n",
    "3. The rest is the same as the accuracy calculations during training, except that in this case, the code iterates through the test_loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 99.18 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing the progress of the CNN during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the modem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
